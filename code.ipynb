{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Uncertaintiy Modeling + Interactive Learning:\n",
    "\n",
    "* **Implement** a CNN classifier with dropout layers and modify it so that the dropout layers can be left on during inference.  \n",
    "\n",
    "* Then **compare** the performance of a couple of uncertainty-based query strategies from the modal active learning library against Entropy-based uncertainty sampling based on dropout-enabled inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from: https://github.com/modAL-python/modAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git cloning and installing the library for AL from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/modAL-python/modAL\n",
    "\n",
    "%pip install git+https://github.com/modAL-python/modAL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement a CNN classifier with dropout layers and modify it so that the dropout layers can be left on during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "%pip install wandb -q\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in into wandab and initialize experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"ML4NLP2\", name=\"run1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration, use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data_loader and pre-processer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "  \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR100(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR100(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# CIFAR100 dataset \n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=64,\n",
    "                              test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = np.load('/Users/gabrieledele/kmnist/kmnist-train-imgs.npz')['arr_0']\n",
    "label_train = np.load('/Users/gabrieledele/kmnist/kmnist-train-labels.npz')['arr_0']\n",
    "\n",
    "mean = img_train.mean()\n",
    "std = img_train.std()\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_Dropout(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_enabled=True):\n",
    "        super(VGG16_Dropout, self).__init__()\n",
    "        self.dropout_enabled = dropout_enabled\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),  # Adjust dropout probability as needed\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),  # Adjust dropout probability as needed\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dropout only during training if enabled\n",
    "        if self.dropout_enabled:\n",
    "            x = nn.functional.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model with dropout enabled during training\n",
    "vgg16_dropout_model = VGG16_Dropout(num_classes=10, dropout_enabled=True)\n",
    "\n",
    "# Print the model architecture\n",
    "print(vgg16_dropout_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "  cost_function = torch.nn.CrossEntropyLoss()\n",
    "  return cost_function\n",
    "\n",
    "def get_optimizer(net, lr, wd, momentum):\n",
    "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modAl needs a wrapper for the cnn to work bc it emulates a scikit architecture?? kinda like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "# create the classifier\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = NeuralNetClassifier(VGG16_Dropout,\n",
    "                                 criterion=nn.CrossEntropyLoss,\n",
    "                                 optimizer=torch.optim.SGD,\n",
    "                                 train_split=None,\n",
    "                                 verbose=1,\n",
    "                                 device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import KMNIST\n",
    "\n",
    "\n",
    "kmnist_data = KMNIST(root='./data', download=True, transform=ToTensor())\n",
    "dataloader = DataLoader(kmnist_data, shuffle=True, batch_size=70000)\n",
    "X, y = next(iter(dataloader))\n",
    "\n",
    "# read training data\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)\n",
    "\n",
    "# assemble initial data\n",
    "n_initial = 1000\n",
    "initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "\n",
    "# generate the pool\n",
    "# remove the initial data from the training dataset\n",
    "X_pool = np.delete(X_train, initial_idx, axis=0)[:6000]\n",
    "y_pool = np.delete(y_train, initial_idx, axis=0)[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
